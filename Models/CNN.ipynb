{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AiUg1dbQ3pn"
      },
      "source": [
        "#INSTRUCTIONS TO EXECUTE THE NOTEBOOK\n",
        "1. Execute all the cells in a sequential manner. Don't jump around and execute cells.\n",
        "2. After you are done executing and saving results (Training and Inferencing) for one activation function, restart session, otherwise the notebook won't run in the second attempt.\n",
        "3. Just confirm once after saving results that the results are being saved in the correct directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx-xs0GWD0d5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from time import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from math import floor\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccw8fvwVmHBj"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC_kbmfoI-qu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtI9x3IFqidm"
      },
      "outputs": [],
      "source": [
        "Year = input(\"Year: \")\n",
        "Area = input(\"Area: \")\n",
        "Ablation = input(\"Ablation(0 or 1): \")\n",
        "activation_fn_list =  ['ReLU','LeakyReLU','SwishReLU','z^2cos(z)','DSU','GCU','SSU']\n",
        "activation_fn = input(f\"Input of Activation Function from the list {activation_fn_list}: \")\n",
        "generalizing = input(\"Generalizing(0 or 1): \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apNLzbnBYH2a"
      },
      "outputs": [],
      "source": [
        "img_path = f'/content/drive/MyDrive/{Area}_{Year}.tif'\n",
        "img_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv-h9YoUJAeM"
      },
      "outputs": [],
      "source": [
        "if Ablation == '0':\n",
        "  feats = 'NA'\n",
        "  df_train = pd.read_csv(f'/content/drive/MyDrive/training_processed_{Area}_{Year}.csv',index_col=0)\n",
        "  df_test = pd.read_csv(f'/content/drive/MyDrive/testing_processed_{Area}_{Year}.csv',index_col=0)\n",
        "else:\n",
        "  feats = input(\"Features(s,si,st): \")\n",
        "  df_train = pd.read_csv(f'/content/drive/MyDrive/training_{feats}_{Area}_{Year}.csv',index_col=0)\n",
        "  df_test = pd.read_csv(f'/content/drive/MyDrive/testing_{feats}_{Area}_{Year}.csv',index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxkJz4hf8tqz"
      },
      "outputs": [],
      "source": [
        "if Ablation=='0':\n",
        "  sample = pd.read_csv(f'/content/drive/MyDrive/processed_data/training_processed_{Area}_{Year}.csv',index_col=0)\n",
        "else:\n",
        "  sample = pd.read_csv(f'/content/drive/MyDrive/Ablation/training_{feats}_{Area}_{Year}.csv',index_col=0)\n",
        "bands_list = sample.columns.values[1:]\n",
        "print(bands_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NiKqKbjATj2"
      },
      "outputs": [],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA1o-80MJWJa"
      },
      "outputs": [],
      "source": [
        "#Shuffling data\n",
        "train_df = df_train.sample(frac=1).reset_index(drop=True)\n",
        "test_df = df_test.sample(frac=1).reset_index(drop=True)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7AzgUcKLM93"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjTPx527grxL"
      },
      "outputs": [],
      "source": [
        "y_train = train_df['Class']\n",
        "y_test = test_df['Class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHZCSYDrZ_dy"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpMMvZGuX5bt"
      },
      "outputs": [],
      "source": [
        "y_test.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi-sANw1KZaM"
      },
      "outputs": [],
      "source": [
        "y_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXcwL9FPKclH"
      },
      "outputs": [],
      "source": [
        "y_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqGA_ejEGinb"
      },
      "outputs": [],
      "source": [
        "classes = y_train.unique()\n",
        "num_class = y_train.nunique()\n",
        "class2 = y_test.unique()\n",
        "\n",
        "print(f\"Classes are - {classes}\")\n",
        "print(f\"Number of classes are - {num_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAasDpgPwjGc"
      },
      "outputs": [],
      "source": [
        "train_df.drop(columns=['Class'],inplace=True)\n",
        "test_df.drop(columns=['Class'],inplace=True)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szS8Q7_AHPPB"
      },
      "outputs": [],
      "source": [
        "class_list = [\"Water\",\"Urban\",\"Vegetation\",\"Barren\"]\n",
        "class_dict = {}\n",
        "for i in class2:\n",
        "  class_dict[i] = class_list[i]\n",
        "print(class_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-PcYSHUtyQF"
      },
      "source": [
        "# DATA LOADERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNWF3qNwK11n"
      },
      "outputs": [],
      "source": [
        "class CSVDataset(Dataset):\n",
        "    def __init__(self, dataframe, target_column, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): DataFrame containing the data.\n",
        "            target_column (str): Name of the target column. If None, assumes no labels (for inference).\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.target_column = target_column\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # if torch.is_tensor(idx):\n",
        "        #     idx = idx.tolist()\n",
        "\n",
        "        # Get the data (features) as a tensor\n",
        "\n",
        "        features = self.dataframe.iloc[idx].values\n",
        "        label = self.target_column.iloc[idx]\n",
        "\n",
        "        features = torch.tensor(features, dtype=torch.float32)  # Convert features to a tensor\n",
        "        features = torch.reshape(features,(1,-1))\n",
        "        label = torch.tensor(label, dtype=torch.long)  # Convert label to a tensor\n",
        "\n",
        "        if self.transform:\n",
        "          features = self.transform(features)\n",
        "\n",
        "        return features, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFVBjvLjt7ee"
      },
      "outputs": [],
      "source": [
        "train_dataset = CSVDataset(train_df,y_train)\n",
        "test_dataset = CSVDataset(test_df,y_test)\n",
        "#After this you can also use random_split to split the dataset, however, we have already defined the train and test dataset during data collection stage\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm-Lrfphb8dx"
      },
      "outputs": [],
      "source": [
        "train_dataset = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_dataset = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKcqyCYeehK9"
      },
      "outputs": [],
      "source": [
        "print(f\"Length of the Training Data Batches {len(train_dataset)}\")\n",
        "input_features =0\n",
        "for input,labels in train_dataset:\n",
        "  input_features = input.shape[2]\n",
        "  print(f\"Dimensions of the first batch {input.shape}\")\n",
        "  print(f\"Dimensions of the first batch's labels {labels.shape}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSc1FgVmFZQz"
      },
      "source": [
        "\n",
        "\n",
        "# Modelling Phase\n",
        "* Here we are creating a 1D CNN with input dimensions as above, we will scale it up later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGitgCe6EIct"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyZ7PjQkEHcb"
      },
      "outputs": [],
      "source": [
        "class SincActivation(nn.Module):\n",
        "    def forward(self, input):\n",
        "        # Implementing sinc(z - pi)\n",
        "        return torch.sin(input - torch.pi) / (input - torch.pi + 1e-9)  # Added epsilon to avoid division by zero\n",
        "\n",
        "class ZCosActivation(nn.Module):\n",
        "    def forward(self, input):\n",
        "        # z * cos(z)\n",
        "        return input * torch.cos(input)\n",
        "\n",
        "class SincSubActivation(nn.Module):\n",
        "    def forward(self, input):\n",
        "        # 2 * (sinc(z - pi) - sinc(z + pi))\n",
        "        sinc_left = torch.sin(input - torch.pi) / (input - torch.pi + 1e-9)\n",
        "        sinc_right = torch.sin(input + torch.pi) / (input + torch.pi + 1e-9)\n",
        "        return 2 * (sinc_left - sinc_right)\n",
        "\n",
        "class zsqcosz(nn.Module):\n",
        "  def forward(self,input):\n",
        "    return input**2 * torch.cos(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqGsr_mdrfHX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Simple1DCNN(nn.Module):\n",
        "    def __init__(self,activation_fn):\n",
        "        super(Simple1DCNN, self).__init__()\n",
        "\n",
        "        # Define the CNN layers\n",
        "        self.conv1a = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1)  # 1 input channel, 16 output channels\n",
        "        self.conv1b = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1)  # 1 input channel, 16 output channels\n",
        "        self.bn1a = nn.BatchNorm1d(16)  # Batch normalization after first convolution layer\n",
        "        self.bn1b = nn.BatchNorm1d(32)  # Batch normalization after second convolution layer\n",
        "\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)  # First pooling layer\n",
        "\n",
        "        self.conv2a = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1)  # 16 input channels, 32 output channels\n",
        "        self.bn2a = nn.BatchNorm1d(64)  # Batch normalization after third convolution layer\n",
        "\n",
        "        self.conv2b = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1)  # 16 input channels, 32 output channels\n",
        "        self.bn2b = nn.BatchNorm1d(128)  # Batch normalization after fourth convolution layer\n",
        "\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)  # Second pooling layer\n",
        "\n",
        "\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Takes in input of any shape, gives us the output of our desired shape by averaging the values adaptively (Based upon the input dimensions and output dimensions)\n",
        "        self.fc1 = nn.Linear(128, 64)  # Fully connected layer to output 4 classes\n",
        "        self.bn_fc1 = nn.BatchNorm1d(64)  # Batch normalization before dropout in fully connected layers\n",
        "\n",
        "        self.fc2 = nn.Linear(64, 32)  # Fully connected layer to output 4 classes\n",
        "        self.bn_fc2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.fc3 = nn.Linear(32,4)\n",
        "\n",
        "        self.activation = activation_fn\n",
        "        # Activation and softmax layers\n",
        "        if(activation_fn == 'ReLU'):\n",
        "          self.activation = nn.ReLU()\n",
        "        elif(activation_fn == 'LeakyReLU'):\n",
        "          self.activation = nn.LeakyReLU()\n",
        "        elif(activation_fn == 'SwishReLU'):\n",
        "          self.activation = nn.SiLU()\n",
        "        elif(activation_fn == 'z^2cos(z)'):\n",
        "          self.activation = SincSubActivation()\n",
        "        elif(activation_fn == 'DSU'):\n",
        "          self.activation = SincSubActivation()\n",
        "        elif(activation_fn == 'GCU'):\n",
        "          self.activation = SincSubActivation()\n",
        "        else:\n",
        "          self.activation = SincActivation()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)  # Softmax activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through conv layer, ReLU, and pooling\n",
        "\n",
        "       # Convolution, BatchNorm, Activation, Pooling, Dropout\n",
        "        x = self.activation(self.conv1a(x))\n",
        "        x = self.activation(self.conv1b(x))\n",
        "        x = self.pool1(x)\n",
        "        x=self.drop(x)\n",
        "\n",
        "        x = self.activation(self.conv2a(x))\n",
        "        x = self.activation(self.conv2b(x))\n",
        "        x = self.pool2(x)\n",
        "        x=self.drop(x)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        # x = self.global_avg_pool(x)\n",
        "\n",
        "        # Flatten for the fully connected layer\n",
        "        x = x.view(-1, 128)\n",
        "\n",
        "        # Fully connected layers with BatchNorm and Dropout\n",
        "        x = self.drop(self.activation(self.fc1(x)))\n",
        "        x = self.drop(self.activation(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uTIHgB1aqM8"
      },
      "source": [
        "# SMALL CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN8hocAYapcq"
      },
      "outputs": [],
      "source": [
        "# # @title DON'T RUN IF BIG CNN IS RUNNING\n",
        "# class Simple1DCNN(nn.Module):\n",
        "#     def __init__(self,activation_fn):\n",
        "#         super(Simple1DCNN, self).__init__()\n",
        "\n",
        "#         # Define the CNN layers\n",
        "#         self.conv1a = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1)  # 1 input channel, 16 output channels\n",
        "\n",
        "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)  # First pooling layer\n",
        "\n",
        "#         self.conv2a = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1)  # 16 input channels, 32 output channels\n",
        "\n",
        "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)  # Second pooling layer\n",
        "\n",
        "\n",
        "#         self.drop = nn.Dropout(0.2)\n",
        "#         self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Takes in input of any shape, gives us the output of our desired shape by averaging the values adaptively (Based upon the input dimensions and output dimensions)\n",
        "#         self.fc1 = nn.Linear(32, 4)  # Fully connected layer to output 4 classes\n",
        "#         # self.fc2 = nn.Linear(64, 32)  # Fully connected layer to output 4 classes\n",
        "#         # self.fc3 = nn.Linear(32,4)\n",
        "\n",
        "#         self.activation = activation_fn\n",
        "#         # Activation and softmax layers\n",
        "#         if(activation_fn == 'ReLU'):\n",
        "#           self.activation = nn.ReLU()\n",
        "#         elif(activation_fn == 'LeakyReLU'):\n",
        "#           self.activation = nn.LeakyReLU()\n",
        "#         elif(activation_fn == 'SwishReLU'):\n",
        "#           self.activation = nn.SiLU()\n",
        "#         elif(activation_fn == 'z^2cos(z)'):\n",
        "#           self.activation = zsqcosz()\n",
        "#         elif(activation_fn == 'DSU'):\n",
        "#           self.activation = SincSubActivation()\n",
        "#         elif(activation_fn == 'GCU'):\n",
        "#           self.activation = ZCosActivation()\n",
        "#         else:\n",
        "#           self.activation = SincActivation()\n",
        "\n",
        "#         self.softmax = nn.Softmax(dim=1)  # Softmax activation\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Pass through conv layer, ReLU, and pooling\n",
        "\n",
        "#         x = self.activation(self.conv1a(x))\n",
        "#         x = self.pool1(x)\n",
        "\n",
        "#         x = self.activation(self.conv2a(x))\n",
        "#         x = self.pool2(x)\n",
        "\n",
        "#         # Global Average Pooling\n",
        "#         x = self.global_avg_pool(x)\n",
        "\n",
        "\n",
        "#         # Flatten the output for the fully connected layer\n",
        "#         x = x.view(-1, 32)  # After global average pooling, output shape is [batch_size, 32, 1] --> Therefore we reshape it\n",
        "#         # Fully connected layer\n",
        "#         x = self.fc1(x)\n",
        "#         # x = self.drop(x)\n",
        "#         # x = self.activation(self.fc2(x))\n",
        "#         # x = self.drop(x)\n",
        "#         # x = self.fc3(x)\n",
        "#         x = self.softmax(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pajeek2yykB3"
      },
      "source": [
        "# For Four Features Ablation Study Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evcbSO_se24U"
      },
      "outputs": [],
      "source": [
        "# class Simple1DCNNAblation(nn.Module):\n",
        "#     def __init__(self, input_dim,activation_fn):\n",
        "#         super(Simple1DCNNAblation, self).__init__()\n",
        "\n",
        "#         # Initialize an empty ModuleList to hold CNN layers\n",
        "#         self.cnn_layers = nn.ModuleList()\n",
        "\n",
        "#         # Initial configuration\n",
        "#         in_channels = 1  # Starting input channels\n",
        "#         out_channels = 0  # Starting output channels\n",
        "#         kernel_size = 3\n",
        "#         stride = 1\n",
        "#         pool_size = 2\n",
        "\n",
        "#         current_dim = input_dim  # Track current dimension of the feature map\n",
        "#         while current_dim >= kernel_size:\n",
        "#             # Add Conv layer\n",
        "#             out_channels+=16\n",
        "#             self.cnn_layers.append(\n",
        "#                 nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "#                 nn.BatchNorm1d(out_channels)\n",
        "#             )\n",
        "#             in_channels=out_channels\n",
        "#             out_channels+=16\n",
        "#             self.cnn_layers.append(\n",
        "#                 nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "#                 nn.BatchNorm1d(out_channels)\n",
        "\n",
        "#             )\n",
        "\n",
        "#             # Update dimensions and channel numbers for the next layer\n",
        "#             current_dim = floor((current_dim - (kernel_size - 1) - 1) / pool_size)  # Pooling reduces the dimension\n",
        "#             in_channels = out_channels  # Input channels for the next layer is the output channels of this layer\n",
        "#               # Incrementally increase out_channels for each added CNN layer\n",
        "\n",
        "#         self.fc = nn.ModuleList()\n",
        "#         self.drop = nn.Dropout(0.2)\n",
        "#         while(out_channels!=32):\n",
        "#           self.fc.append(nn.Linear(out_channels,out_channels/2))\n",
        "#           out_channels = out_channels/2\n",
        "\n",
        "#         self.pool1d = nn.MaxPool1d(kernel_size=pool_size, stride=pool_size)\n",
        "#         self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Takes in input of any shape, gives us the output of our desired shape by averaging the values adaptively (Based upon the input dimensions and output dimensions)\n",
        "#         self.drop = nn.Dropout(0.2)\n",
        "#         self.fc3 = nn.Linear(out_channels, 4)\n",
        "#         self.activation=None\n",
        "#         if(activation_fn == 'ReLU'):\n",
        "#           self.activation = nn.ReLU()\n",
        "#         elif(activation_fn == 'LeakyReLU'):\n",
        "#           self.activation = nn.LeakyReLU()\n",
        "#         elif(activation_fn == 'SwishReLU'):\n",
        "#           self.activation = nn.SiLU()\n",
        "#         elif(activation_fn == 'z^2cos(z)'):\n",
        "#           self.activation = zsqcosz()\n",
        "#         elif(activation_fn == 'DSU'):\n",
        "#           self.activation = SincSubActivation()\n",
        "#         elif(activation_fn == 'GCU'):\n",
        "#           self.activation = ZCosActivation()\n",
        "#         else:\n",
        "#           self.activation = SincActivation()\n",
        "#         self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         #Dynamically pool the output from the Conv layers\n",
        "#         for i,layer in enumerate(self.cnn_layers):\n",
        "#           if(!isinstance(layer,nn.BatchNorm1d())):\n",
        "#             x = self.activation(layer(x))\n",
        "#           else:\n",
        "#             x = layer(x)\n",
        "#             x = self.activation(x)\n",
        "#             if(i%2==0 & i!=0):\n",
        "#               if(x.size()[2]>=2):\n",
        "#                 x = self.pool1d(x)\n",
        "#                 x = self.drop(x)\n",
        "\n",
        "#         # Global Average Pooling\n",
        "#         x = self.global_avg_pool(x)\n",
        "\n",
        "#         # Flatten the output for fully connected layers\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         for layer in self.fc:\n",
        "#           x = self.activation(layer(x))\n",
        "#           x = self.drop(x)\n",
        "#         x = self.softmax(self.fc3(x))\n",
        "\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1jzp2l2RKbW"
      },
      "outputs": [],
      "source": [
        "from math import floor\n",
        "\n",
        "class Simple1DCNNAblation(nn.Module):\n",
        "    def __init__(self, input_dim, activation_fn):\n",
        "        super(Simple1DCNNAblation, self).__init__()\n",
        "\n",
        "        self.cnn_layers = nn.ModuleList()\n",
        "\n",
        "        in_channels = 1\n",
        "        out_channels = 16  # Start with a valid number of output channels\n",
        "        kernel_size = 3\n",
        "        stride = 1\n",
        "        pool_size = 2\n",
        "\n",
        "        current_dim = input_dim  # Track current dimension of the feature map\n",
        "        while current_dim >= kernel_size:\n",
        "            self.cnn_layers.append(nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride))\n",
        "\n",
        "            # Update in_channels and out_channels for the next layer\n",
        "            in_channels = out_channels\n",
        "            out_channels += 16\n",
        "\n",
        "            # Update the current dimension after pooling\n",
        "            current_dim = floor((current_dim - (kernel_size - 1) - 1) / pool_size)\n",
        "            if current_dim < kernel_size:\n",
        "                break\n",
        "\n",
        "        # Save the final output size after CNN layers and pooling\n",
        "        self.final_cnn_output_size = in_channels * current_dim\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        fc_input_size = self.final_cnn_output_size\n",
        "        while fc_input_size > 32:\n",
        "            self.fc_layers.append(nn.Linear(fc_input_size, fc_input_size // 2))\n",
        "            fc_input_size //= 2\n",
        "\n",
        "        self.pool1d = nn.MaxPool1d(kernel_size=pool_size, stride=pool_size)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(fc_input_size, 16)\n",
        "        self.fc4 = nn.Linear(16, 4)\n",
        "\n",
        "        # Define activation function\n",
        "        if activation_fn == 'ReLU':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation_fn == 'LeakyReLU':\n",
        "            self.activation = nn.LeakyReLU()\n",
        "        elif activation_fn == 'SwishReLU':\n",
        "            self.activation = nn.SiLU()\n",
        "        elif activation_fn == 'z^2cos(z)':\n",
        "            self.activation = SincSubActivation()\n",
        "        elif activation_fn == 'DSU':\n",
        "            self.activation = SincSubActivation()\n",
        "        elif activation_fn == 'GCU':\n",
        "            self.activation = SincSubActivation()\n",
        "        else:\n",
        "            self.activation = SincActivation()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.cnn_layers)):\n",
        "            x = self.activation(self.cnn_layers[i](x))  # Apply activation after convolution\n",
        "\n",
        "            if x.size(2) >= 2:\n",
        "                x = self.pool1d(x)  # Apply pooling\n",
        "                x = self.drop(x)   # Apply dropout\n",
        "\n",
        "        # Global Average Pooling\n",
        "        x = self.global_avg_pool(x)\n",
        "\n",
        "        # Flatten for fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "        for layer in self.fc_layers:\n",
        "            x = self.activation(layer(x))\n",
        "            x = self.drop(x)\n",
        "\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.softmax(self.fc4(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FqgdvmlG76Q"
      },
      "outputs": [],
      "source": [
        "if(feats=='NA'):\n",
        "  model = Simple1DCNN(activation_fn).to(device)\n",
        "  print(model)\n",
        "  print(summary(model,(1,19)))\n",
        "else:\n",
        "  # Create an instance of the model\n",
        "  model = Simple1DCNNAblation(input_features,activation_fn).to(device)\n",
        "  print(model)\n",
        "  print(summary(model,(1,input_features)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpj_rGvqUJvx"
      },
      "outputs": [],
      "source": [
        "def validate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    print(f'Validation Loss: {avg_loss:.4f}')\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxKJKhGZbBdm"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calculate class weights based on the distribution of the labels\n",
        "def get_class_weights(labels):\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "    return torch.tensor(class_weights, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwAuv81ebCsK"
      },
      "outputs": [],
      "source": [
        "all_labels = []\n",
        "\n",
        "# Gather all labels from the train_loader to compute class weights\n",
        "for inputs, labels in train_dataset:\n",
        "    all_labels.extend(labels.numpy())\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = get_class_weights(all_labels)\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# Move weights to the same device as the model\n",
        "class_weights = class_weights.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GEEBPZVVkHp"
      },
      "outputs": [],
      "source": [
        "path_name=None\n",
        "if(Ablation=='0'):\n",
        "  path_name = 'Non-Ablated_Data'\n",
        "else:\n",
        "  if(feats=='s'):\n",
        "    path_name = 'Spectral_Features'\n",
        "  elif(feats=='si'):\n",
        "    path_name = 'Spectral+Indices'\n",
        "  else:\n",
        "    path_name = 'Spectral+Textures'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3NT6QDuIRJS"
      },
      "outputs": [],
      "source": [
        "if(generalizing=='0'):\n",
        "  generalizing='Non-generalizability'\n",
        "else:\n",
        "  generalizing='Generalizability'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOyo4Hrwctjm"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUoXDuKkF4nF"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights)  # Assume 4 classes for this example\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion,optimizer,epochs=101,patience=5):\n",
        "    training_logs = {'Training_Loss':[],'Training_Accuracy':[],'Validation_Loss':[]}\n",
        "    model.train()  # Set model to training mode\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "    best_loss = float('inf')  # Initialize best loss for early stopping\n",
        "    patience_counter = 0  # Counter for early stopping\n",
        "    train_losses = []  # List to store the loss values for each epoch\n",
        "    train_accuracy = [] #List to store training accuracy\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total=0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "            # Get predictions and calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate accuracy for this epoch\n",
        "        accuracy = 100 * correct / total\n",
        "            # if i % 10 == 9:    # Print every 10 batches\n",
        "            #     print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 10:.4f}')\n",
        "        # Compute average loss for the epoch\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)  # Store epoch loss\n",
        "        train_accuracy.append(accuracy)\n",
        "        training_logs['Training_Loss'].append(epoch_loss)\n",
        "        training_logs['Training_Accuracy'].append(accuracy)\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Average Loss: {epoch_loss:.4f}, Average Accuracy {accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "        # Validate the model after each epoch\n",
        "        val_loss = validate(model, test_loader, criterion)\n",
        "        training_logs['Validation_Loss'].append(val_loss)\n",
        "        scheduler.step(val_loss)  # Step the scheduler\n",
        "\n",
        "\n",
        "### Early Stopping commented for Experimentation purposes -\n",
        "\n",
        "\n",
        "        # # Early stopping\n",
        "        # if val_loss < best_loss:\n",
        "        #     best_loss = val_loss\n",
        "        #     patience_counter = 0  # Reset counter\n",
        "        #     print(f\"Validation loss improved to {best_loss:.4f}.\")\n",
        "        # else:\n",
        "        #     patience_counter += 1\n",
        "        #     print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
        "        #     if patience_counter >= patience:\n",
        "        #         print(\"Early stopping triggered.\")\n",
        "        #         break\n",
        "\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot Training Loss\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Loss', color='blue', linewidth=2, marker='o')\n",
        "    plt.title(f'Training Loss - Activation Function: {activation_fn}', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.grid(False)  # Remove grid\n",
        "    plt.legend(fontsize=12)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracy, label='Accuracy', color='green', linewidth=2, marker='o')\n",
        "    plt.title(f'Training Accuracy - Activation Function: {activation_fn}', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "    plt.grid(False)  # Remove grid\n",
        "    plt.legend(fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure\n",
        "    os.makedirs(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}', exist_ok=True)\n",
        "    plt.savefig(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/training_plot.png', dpi=300)\n",
        "    plt.show()\n",
        "    return training_logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoKIS8ZZUh-h"
      },
      "outputs": [],
      "source": [
        "os.makedirs(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/demo.txt', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLLEC_Ueb9_F"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n",
        "\n",
        "\n",
        "def inference(model, test_loader, num_classes,name):\n",
        "    test_statistics = {}\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients during inference\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Store predictions and true labels for further analysis\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Convert to numpy arrays for metrics computation\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    classes = np.unique(all_predictions)\n",
        "    print(classes)\n",
        "    # print(np.unique(all_predictions))\n",
        "    # print(np.unique(all_labels))\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the test data: {accuracy:.2f}%')\n",
        "\n",
        "    # Precision, Recall, and F1-Score (average='weighted' to consider class imbalance)\n",
        "\n",
        "    #As Precision, Recall and F1 is calculated per class (One vs all) and then aggregated, so weighted parameter will aggregate the values using the sample size and give weights accordingly\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    print(f'Precision: {precision:.2f}')\n",
        "    print(f'Recall: {recall:.2f}')\n",
        "    print(f'F1-Score: {f1:.2f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f'Confusion Matrix:\\n{cm}')\n",
        "\n",
        "    # Plot the confusion matrix using seaborn\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    os.makedirs(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}', exist_ok=True)\n",
        "    if(generalizing=='Non-generalizability'):\n",
        "      plt.savefig(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/confusion_matrix.png', dpi=300)\n",
        "    else:\n",
        "      if(name=='Chicago'):\n",
        "        plt.savefig(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/confusion_matrix.png', dpi=300)\n",
        "      else:\n",
        "        plt.savefig(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/confusion_matrix.png', dpi=300)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # User's Accuracy (Precision per class)\n",
        "    user_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "    for i, ua in enumerate(user_accuracy):\n",
        "        print(f\"User's Accuracy for class {i}: {ua:.2f}\")\n",
        "\n",
        "    # Producer's Accuracy (Recall per class)\n",
        "    producer_accuracy = cm.diagonal() / cm.sum(axis=0)\n",
        "    print(f\"Producer Accuracyy {producer_accuracy}\")\n",
        "    print(type(producer_accuracy))\n",
        "    for i, pa in enumerate(producer_accuracy):\n",
        "        print(f\"Producer's Accuracy for class {i}: {pa:.2f}\")\n",
        "\n",
        "    # Kappa Score\n",
        "    kappa = cohen_kappa_score(all_labels, all_predictions)\n",
        "    print(f'Kappa Score: {kappa:.2f}')\n",
        "\n",
        "    test_statistics[\"Test Accuracy\"] = [accuracy]\n",
        "    test_statistics[\"Precision\"] = [precision]\n",
        "    test_statistics[\"Recall\"] = [recall]\n",
        "    test_statistics[\"F1-Score\"] = [f1]\n",
        "    test_statistics[\"Kappa Score\"] = [kappa]\n",
        "    test_statistics[\"Confusion Matrix\"] = [cm.tolist()]\n",
        "    # print(cm.tolist())\n",
        "    df = pd.DataFrame(test_statistics)\n",
        "    user_accuracy_cols = {f'User Accuracy Class {i}': [acc] for i, acc in enumerate(user_accuracy.tolist())}\n",
        "    print(user_accuracy_cols)\n",
        "    producer_accuracy_cols = {f'Producer Accuracy Class {i}': [acc] for i, acc in enumerate(producer_accuracy.tolist())}\n",
        "\n",
        "    # Combine all columns into a single DataFrame\n",
        "    df = pd.concat([df, pd.DataFrame({**user_accuracy_cols, **producer_accuracy_cols})], axis=1)\n",
        "\n",
        "    test_statistics[\"User Accuracy\"] = user_accuracy.tolist()\n",
        "    test_statistics[\"Producer Accuracy\"] = producer_accuracy.tolist()\n",
        "    test_statistics[\"Confusion Matrix\"] = [cm]\n",
        "    # Open a new text file and write test statistics\n",
        "    if(generalizing=='Non-generalizability'):\n",
        "      save_path = f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/test_logs.txt'\n",
        "    else:\n",
        "      if(name=='Chicago'):\n",
        "        save_path = f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/test_logs.txt'\n",
        "      else:\n",
        "        save_path = f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/Generalizetest_logs.txt'\n",
        "\n",
        "    with open(save_path, 'w') as file:\n",
        "        for key, value in test_statistics.items():\n",
        "            # Formatting each line\n",
        "            if isinstance(value, list):  # for lists like User and Producer Accuracy\n",
        "                value = ', '.join(map(str, value))\n",
        "            elif isinstance(value, list):  # for lists like Confusion Matrix\n",
        "                value = '\\n' + '\\n'.join(['\\t'.join(map(str, row)) for row in value])\n",
        "            file.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HuT9DTC5LylH"
      },
      "outputs": [],
      "source": [
        "# @title Previous Inference CODE\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
        "#                              confusion_matrix, cohen_kappa_score)\n",
        "\n",
        "# def inference(model, test_loader, num_classes):\n",
        "#     test_statistics = {}\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     all_labels = []\n",
        "#     all_predictions = []\n",
        "\n",
        "#     with torch.no_grad():  # No need to track gradients during inference\n",
        "#         for inputs, labels in test_loader:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             # Get model outputs\n",
        "#             outputs = model(inputs)\n",
        "\n",
        "#             # Get predictions\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "#             # Store predictions and true labels for further analysis\n",
        "#             all_predictions.extend(predicted.cpu().numpy())\n",
        "#             all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#             # Calculate accuracy\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     # Convert to numpy arrays for metrics computation\n",
        "#     all_labels = np.array(all_labels).ravel()  # Ensure it's 1D\n",
        "#     all_predictions = np.array(all_predictions).ravel()  # Ensure it's 1D\n",
        "\n",
        "#     # Check shapes\n",
        "#     print(\"Shape of all_labels:\", all_labels.shape)\n",
        "#     print(\"Shape of all_predictions:\", all_predictions.shape)\n",
        "\n",
        "#     # Accuracy\n",
        "#     accuracy = 100 * correct / total\n",
        "#     print(f'Accuracy of the network on the test data: {accuracy:.2f}%')\n",
        "\n",
        "#     # Precision, Recall, and F1-Score\n",
        "#     precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "#     recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "#     f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "#     print(f'Precision: {precision:.2f}')\n",
        "#     print(f'Recall: {recall:.2f}')\n",
        "#     print(f'F1-Score: {f1:.2f}')\n",
        "\n",
        "#     # Confusion Matrix\n",
        "#     cm = confusion_matrix(all_labels, all_predictions)\n",
        "#     print(f'Confusion Matrix:\\n{cm}')\n",
        "\n",
        "#     # Ensure cm is not empty\n",
        "#     if cm.size == 0:\n",
        "#         raise ValueError(\"Confusion matrix is empty. Check your predictions and labels.\")\n",
        "\n",
        "#     # Plot the confusion matrix using seaborn\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class2, yticklabels=class2)\n",
        "#     plt.title('Confusion Matrix')\n",
        "#     plt.xlabel('Predicted Label')\n",
        "#     plt.ylabel('True Label')\n",
        "#     plt.savefig(f'confusion_matrix.png', dpi=300)  # Save the plot\n",
        "#     plt.show()\n",
        "\n",
        "#     # User's Accuracy (Precision per class)\n",
        "#     user_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "#     for i, ua in enumerate(user_accuracy):\n",
        "#         print(f\"User's Accuracy for class {i}: {ua:.2f}\")\n",
        "\n",
        "#     # Producer's Accuracy (Recall per class)\n",
        "#     producer_accuracy = cm.diagonal() / cm.sum(axis=0)\n",
        "#     for i, pa in enumerate(producer_accuracy):\n",
        "#         print(f\"Producer's Accuracy for class {i}: {pa:.2f}\")\n",
        "\n",
        "#     # Kappa Score\n",
        "#     kappa = cohen_kappa_score(all_labels, all_predictions)\n",
        "#     print(f'Kappa Score: {kappa:.2f}')\n",
        "\n",
        "#     test_statistics[\"Test Accuracy\"] = accuracy\n",
        "#     test_statistics[\"Precision\"] = precision\n",
        "#     test_statistics[\"Recall\"] = recall\n",
        "#     test_statistics[\"F1-Score\"] = f1\n",
        "#     test_statistics[\"User Accuracy\"] = user_accuracy\n",
        "#     test_statistics[\"Producer Accuracy\"] = producer_accuracy\n",
        "#     test_statistics[\"Kappa Score\"] = kappa\n",
        "#     test_statistics[\"Confusion Matrix\"] = cm.tolist()\n",
        "\n",
        "#     return test_statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42mLzgwfvVbL"
      },
      "source": [
        "#TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K91qETmqLIcY"
      },
      "outputs": [],
      "source": [
        "# Train the model for 100 epochs\n",
        "\n",
        "train_logs = train(model, train_dataset, test_dataset, criterion, optimizer,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sxxYJJHMCedn"
      },
      "outputs": [],
      "source": [
        "# train_logs = pd.DataFrame(train_logs)\n",
        "# train_logs.head()\n",
        "# torch.save(model.state_dict(),  f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/best_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KcrdVHsAppIA"
      },
      "outputs": [],
      "source": [
        "# model_path = f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/best_model.pt'\n",
        "# model = Simple1DCNN(activation_fn)\n",
        "# model.load_state_dict(torch.load(model_path))\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCBrYVKlwS3O"
      },
      "source": [
        "#INFERENCING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fkm3f4i4Qj1k"
      },
      "outputs": [],
      "source": [
        "#Inferencing\n",
        "test_logs = inference(model, test_dataset,4,\"Pune\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VJu8oOwR1hnw"
      },
      "outputs": [],
      "source": [
        "print(test_logs.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kmwoZR4mQb1L"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "train_logs = pd.DataFrame(train_logs)\n",
        "train_logs.to_csv(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/train_logs.csv',index=False)\n",
        "\n",
        "# if(generalizing=='0'):\n",
        "test_logs = pd.DataFrame(test_logs)\n",
        "test_logs.to_csv(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/test_logs.csv',index=False)\n",
        "# else:\n",
        "#   test_logs = pd.DataFrame(test_logs)\n",
        "#   test_logs.to_csv(f'/content/drive/MyDrive/Results/CNN/{generalizing}/{path_name}/{activation_fn}/test_logs_chicago_inference.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGalPRvrUqhp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
